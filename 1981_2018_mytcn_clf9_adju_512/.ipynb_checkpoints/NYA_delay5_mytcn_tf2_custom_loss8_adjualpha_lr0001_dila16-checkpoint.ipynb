{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190213\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(tf.__file__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work: model  9 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot_ng\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>vex_sign</th>\n",
       "      <th>cave_sign</th>\n",
       "      <th>&lt;Open&gt;</th>\n",
       "      <th>&lt;High&gt;</th>\n",
       "      <th>&lt;Low&gt;</th>\n",
       "      <th>&lt;Volume&gt;</th>\n",
       "      <th>&lt;ParCl&gt;</th>\n",
       "      <th>&lt;FastAvg&gt;</th>\n",
       "      <th>&lt;SlowAvg&gt;</th>\n",
       "      <th>...</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>RPI</th>\n",
       "      <th>PERMIT</th>\n",
       "      <th>PAYEMS</th>\n",
       "      <th>ICSA</th>\n",
       "      <th>AWHMAN</th>\n",
       "      <th>PPIACO</th>\n",
       "      <th>USSLIND</th>\n",
       "      <th>STLFSI</th>\n",
       "      <th>USREC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>827.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>827.50</td>\n",
       "      <td>827.50</td>\n",
       "      <td>827.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>399000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>836.17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>836.17</td>\n",
       "      <td>836.17</td>\n",
       "      <td>836.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-06</th>\n",
       "      <td>836.81</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>836.81</td>\n",
       "      <td>836.81</td>\n",
       "      <td>836.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-07</th>\n",
       "      <td>817.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>817.24</td>\n",
       "      <td>817.24</td>\n",
       "      <td>817.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-08</th>\n",
       "      <td>805.72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>805.72</td>\n",
       "      <td>805.72</td>\n",
       "      <td>805.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close  vex_sign  cave_sign   <Open>   <High>   <Low>   <Volume>  \\\n",
       "Date                                                                           \n",
       "1981-01-02  827.50         0          0   827.50   827.50  827.50          0   \n",
       "1981-01-05  836.17         0          1   836.17   836.17  836.17          0   \n",
       "1981-01-06  836.81         0          1   836.81   836.81  836.81          0   \n",
       "1981-01-07  817.24         1          0   817.24   817.24  817.24          0   \n",
       "1981-01-08  805.72         1          0   805.72   805.72  805.72          0   \n",
       "\n",
       "             <ParCl>   <FastAvg>   <SlowAvg>  ...   INDPRO       RPI  PERMIT  \\\n",
       "Date                                          ...                              \n",
       "1981-01-02       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-05       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-06       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-07       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-08       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "\n",
       "            PAYEMS    ICSA  AWHMAN  PPIACO  USSLIND  STLFSI  USREC  \n",
       "Date                                                                \n",
       "1981-01-02   91037  399000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-05   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-06   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-07   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-08   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r'D:\\python_code\\data\\stock\\^NYA'\n",
    "fname = os.path.join(data_dir, 'NYA 1981 2018 technical_custom_loss4.csv')\n",
    "df = pd.read_csv(fname)\n",
    "df['Date'] = pd.to_datetime(df[\"Date\"])\n",
    "df_idx = df.set_index([\"Date\"], drop=True)\n",
    "df_idx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把順序調換\n",
    "df_idx = df_idx.sort_index(axis=0, ascending=False)\n",
    "df_idx = df_idx.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>vex_sign</th>\n",
       "      <th>cave_sign</th>\n",
       "      <th>&lt;Open&gt;</th>\n",
       "      <th>&lt;High&gt;</th>\n",
       "      <th>&lt;Low&gt;</th>\n",
       "      <th>&lt;Volume&gt;</th>\n",
       "      <th>&lt;ParCl&gt;</th>\n",
       "      <th>&lt;FastAvg&gt;</th>\n",
       "      <th>&lt;SlowAvg&gt;</th>\n",
       "      <th>...</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>RPI</th>\n",
       "      <th>PERMIT</th>\n",
       "      <th>PAYEMS</th>\n",
       "      <th>ICSA</th>\n",
       "      <th>AWHMAN</th>\n",
       "      <th>PPIACO</th>\n",
       "      <th>USSLIND</th>\n",
       "      <th>STLFSI</th>\n",
       "      <th>USREC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>827.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>827.50</td>\n",
       "      <td>827.50</td>\n",
       "      <td>827.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>399000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>836.17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>836.17</td>\n",
       "      <td>836.17</td>\n",
       "      <td>836.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-06</th>\n",
       "      <td>836.81</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>836.81</td>\n",
       "      <td>836.81</td>\n",
       "      <td>836.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-07</th>\n",
       "      <td>817.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>817.24</td>\n",
       "      <td>817.24</td>\n",
       "      <td>817.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-08</th>\n",
       "      <td>805.72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>805.72</td>\n",
       "      <td>805.72</td>\n",
       "      <td>805.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.4688</td>\n",
       "      <td>5723.536</td>\n",
       "      <td>1221</td>\n",
       "      <td>91037</td>\n",
       "      <td>410000</td>\n",
       "      <td>40.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close  vex_sign  cave_sign   <Open>   <High>   <Low>   <Volume>  \\\n",
       "Date                                                                           \n",
       "1981-01-02  827.50         0          0   827.50   827.50  827.50          0   \n",
       "1981-01-05  836.17         0          1   836.17   836.17  836.17          0   \n",
       "1981-01-06  836.81         0          1   836.81   836.81  836.81          0   \n",
       "1981-01-07  817.24         1          0   817.24   817.24  817.24          0   \n",
       "1981-01-08  805.72         1          0   805.72   805.72  805.72          0   \n",
       "\n",
       "             <ParCl>   <FastAvg>   <SlowAvg>  ...   INDPRO       RPI  PERMIT  \\\n",
       "Date                                          ...                              \n",
       "1981-01-02       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-05       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-06       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-07       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "1981-01-08       0.0         0.0         0.0  ...  52.4688  5723.536    1221   \n",
       "\n",
       "            PAYEMS    ICSA  AWHMAN  PPIACO  USSLIND  STLFSI  USREC  \n",
       "Date                                                                \n",
       "1981-01-02   91037  399000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-05   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-06   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-07   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "1981-01-08   91037  410000    40.1    95.2      0.0     0.0      0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_idx\n",
    "data.plot(y='Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.index.values[-1] - data.index.values[0]\n",
    "days = diff.astype('timedelta64[D]')\n",
    "days = days / np.timedelta64(1, 'D')\n",
    "years = int(days/365)\n",
    "print(\"total data days:\",days)\n",
    "print(\"Total data: %d years\"%years)\n",
    "print(\"80 percent data = 1981 to %d\"%(1981 + int(0.8*years)))\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#切割訓練與測試資料\n",
    "split_date = pd.Timestamp('01-01-2011')\n",
    "\n",
    "train = data.loc[:split_date]\n",
    "test = data.loc[split_date:]\n",
    "test_date = test.index\n",
    "test_date = pd.to_datetime(test_date)\n",
    "train_date = train.index\n",
    "train_date = pd.to_datetime(train_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料正規化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without sc\n",
    "#train_sc = train\n",
    "#test_sc = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df = pd.DataFrame(train_sc,index=train.index,columns=train.columns)\n",
    "test_sc_df = pd.DataFrame(test_sc,index=test.index,columns=test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(-5,-4):\n",
    "    train_sc_df['Y_{}'.format(s)] = train_sc_df['Close'].shift(s)\n",
    "    test_sc_df['Y_{}'.format(s)] = test_sc_df['Close'].shift(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = train_sc_df.dropna().drop('Y', axis=1)\n",
    "X_train = train_sc_df.dropna().drop('Y_-5', axis=1)\n",
    "y_train = train_sc_df.dropna()['Y_-5']\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "vex_sign_train = train.dropna()['vex_sign']\n",
    "vex_sign_train = vex_sign_train[5:]\n",
    "vex_sign_train = vex_sign_train.as_matrix()\n",
    "cave_sign_train = train.dropna()['cave_sign']\n",
    "cave_sign_train = cave_sign_train[5:]\n",
    "cave_sign_train = cave_sign_train.as_matrix()\n",
    "vex_sign_train[0],cave_sign_train[0]=0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vex_sign_train[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vex_sign_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_sc_df.dropna().drop('Y_-5', axis=1)\n",
    "y_test = test_sc_df.dropna().dropna()['Y_-5']\n",
    "X_test = X_test.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "vex_sign_test = test.dropna().dropna()['vex_sign']\n",
    "vex_sign_test = vex_sign_test[5:]\n",
    "vex_sign_test = vex_sign_test.as_matrix()\n",
    "cave_sign_test = test.dropna().dropna()['cave_sign']\n",
    "cave_sign_test = cave_sign_test[5:]\n",
    "cave_sign_test = cave_sign_test.as_matrix()\n",
    "\n",
    "vex_sign_test[0],cave_sign_test[0]=0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train size: (%d x %d)'%(X_train.shape[0], X_train.shape[1]))\n",
    "print('Test size: (%d x %d)'%(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_imagepath ='images/loss/'\n",
    "predict_imagepath ='images/predict/'\n",
    "losspath = 'csv/loss/'\n",
    "if (not (os.path.exists(losspath))):\n",
    "        os.makedirs(losspath)\n",
    "model_dirpath = 'h5/'\n",
    "file_name='file_name'\n",
    "\n",
    "#for func initiate\n",
    "history_model='history_model'\n",
    "input_tensor='input_tensor'\n",
    "vex_sign='vex_sign'\n",
    "cave_sign='cave_sign'\n",
    "y_pred='y_pred'\n",
    "Target_DirPath='Target_DirPath'\n",
    "test_date_trim='test_date_trim'\n",
    "train_date_trim='train_date_trim'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Add,Reshape,Lambda\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping,CSVLogger\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import Input,layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.utils import plot_model\n",
    "K.clear_session()\n",
    "#from tcn import compiled_tcn,TCN\n",
    "\n",
    "def adj_r2_score(r2, n, k):\n",
    "    return 1-((1-r2)*((n-1)/(n-k-1)))\n",
    "\n",
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "val_split_ratio = 0.1\n",
    "penalty=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_architecture(model, file_name):\n",
    "    file_path = 'images/model/{}.png'.format(file_name)\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    plot_model(model, to_file=file_path, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel(model_dirpath=model_dirpath,file_name=file_name):\n",
    "    model = load_model(model_dirpath + file_name + '.h5')\n",
    "    return model\n",
    "def loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(history_model.history.keys())\n",
    "    loss = history_model.history['loss']\n",
    "    val_loss = history_model.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    if (not (os.path.exists(loss_imagepath))):\n",
    "                os.makedirs(loss_imagepath)\n",
    "    plt.savefig(loss_imagepath +  file_name +'_lr0001_dila16_loss.png')\n",
    "def CSV(losspath=losspath, file_name=file_name):\n",
    "    csv_logger = CSVLogger(losspath + file_name + '_lr0001_dila16_log.csv')\n",
    "    return csv_logger\n",
    "def predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign,cave_sign=cave_sign,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name):\n",
    "    from sklearn.metrics import r2_score\n",
    "    y_pred = model.predict([X_tst_t,vex_sign,cave_sign])\n",
    "    #y_pred是三天前就知道，所以往前移三格\n",
    "    y_test_pic = y_test[:]\n",
    "    y_pred_pic = y_pred[5:]\n",
    "    y_test_rsquare = y_test[:]\n",
    "    plt.plot(y_test_pic, label='True')\n",
    "    plt.plot(y_pred_pic, label='pred')\n",
    "    plt.xlabel('Observation')\n",
    "    plt.ylabel('Scaled_Value')\n",
    "    plt.legend()\n",
    "    print(\"y_pred.shape:\",y_pred.shape)\n",
    "    print(\"y_test_rsquare.shape:\",y_test_rsquare.shape)\n",
    "    r2_test = r2_score(y_test_rsquare, y_pred)\n",
    "    print('R-Squared: %f'%(r2_test))\n",
    "    print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "          .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "    if (not (os.path.exists(predict_imagepath))):\n",
    "            os.makedirs(predict_imagepath)\n",
    "    plt.savefig(predict_imagepath +  file_name +'_lr0001_dila16_loss.png')\n",
    "    plt.show()    \n",
    "    return y_pred\n",
    "def save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\\\n",
    "             file_name=file_name,test_date=test_date):\n",
    "    # 把價格縮放解除\n",
    "    ## create empty table with label fields\n",
    "    y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_pred_data_like[:,0] = y_pred[:,0]\n",
    "    ## inverse transform and then select the right field\n",
    "    y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "    #檢查測試資料的維度\n",
    "    yd_size = X_test.shape[0]\n",
    "\n",
    "    # 把價格轉換維度\n",
    "    yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "    #把最後X天刪除(預測X天後)\n",
    "    test_date_trim = np.delete(test_date, np.s_[-5:])\n",
    "\n",
    "    # 製作CSV\n",
    "    AnalysisResult = pd.DataFrame()\n",
    "    Date = pd.Series(test_date_trim)\n",
    "    Close = pd.Series(yd)\n",
    "    Date.name = 'Date'\n",
    "    Close.name = 'Close'\n",
    "\n",
    "    # 因為放在MC要開高低收，所以複製收盤填入\n",
    "    Open = Close.copy()\n",
    "    High = Close.copy()\n",
    "    Low = Close.copy()\n",
    "    Open.name = 'Open'\n",
    "    High.name = 'High'\n",
    "    Low.name = 'Low'\n",
    "\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Close], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "    # 輸出CSV檔案\n",
    "    import os\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "    if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "    AnalysisResult.to_csv(Target_DirPath + file_name+'_lr0001_dila16.csv', mode='w', header=True, index=False)\n",
    "    \n",
    "def save_train_csv(file_name=file_name):\n",
    "    y_train_pred = model.predict([X_tr_t,vex_sign_train,cave_sign_train])\n",
    "    # 把價格縮放解除\n",
    "    ## create empty table with label fields\n",
    "    y_train_pred_data_like = np.zeros(shape=(len(y_train_pred), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_train_pred_data_like[:,0] = y_train_pred[:,0]\n",
    "    ## inverse transform and then select the right field\n",
    "    y_train_pred_data = sc.inverse_transform(y_train_pred_data_like)[:,0]\n",
    "\n",
    "    #檢查測試資料的維度\n",
    "    yd_size = X_train.shape[0]\n",
    "\n",
    "    # 把價格轉換維度\n",
    "    yd = y_train_pred_data.reshape(yd_size,)\n",
    "\n",
    "    #把最後X天刪除(預測X天後)\n",
    "    train_date_trim = np.delete(train_date, np.s_[-5:])\n",
    "\n",
    "    # 製作CSV\n",
    "    AnalysisResult = pd.DataFrame()\n",
    "    Date = pd.Series(train_date_trim)\n",
    "    Close = pd.Series(yd)\n",
    "    Date.name = 'Date'\n",
    "    Close.name = 'Close'\n",
    "\n",
    "    # 因為放在MC要開高低收，所以複製收盤填入\n",
    "    Open = Close.copy()\n",
    "    High = Close.copy()\n",
    "    Low = Close.copy()\n",
    "    Open.name = 'Open'\n",
    "    High.name = 'High'\n",
    "    Low.name = 'Low'\n",
    "\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Close], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "    # 輸出CSV檔案\n",
    "    import os\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master-train/'\n",
    "    if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "    AnalysisResult.to_csv(Target_DirPath + file_name+'_lr0001_dila16_train.csv', mode='w', header=True, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_of_skip_connections='selu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Convolution1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def channel_normalization(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\" Normalize a layer to the maximum activation\n",
    "\n",
    "    This keeps a layers values between zero and one.\n",
    "    It helps with relu's unbounded activation\n",
    "\n",
    "    Args:\n",
    "        x: The layer to normalize\n",
    "\n",
    "    Returns:\n",
    "        A maximal normalized layer\n",
    "    \"\"\"\n",
    "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
    "    out = x / max_values\n",
    "    return out\n",
    "\n",
    "\n",
    "def wave_net_activation(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\"This method defines the activation used for WaveNet\n",
    "\n",
    "    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
    "\n",
    "    Args:\n",
    "        x: The layer we want to apply the activation to\n",
    "\n",
    "    Returns:\n",
    "        A new layer with the wavenet activation applied\n",
    "    \"\"\"\n",
    "    tanh_out = Activation('tanh')(x)\n",
    "    sigm_out = Activation('sigmoid')(x)\n",
    "    return tensorflow.keras.layers.multiply([tanh_out, sigm_out])\n",
    "\n",
    "\n",
    "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
    "    # type: (Layer, int, int, str, int, int, str, float, str) -> Tuple[Layer, Layer]\n",
    "    \"\"\"Defines the residual block for the WaveNet TCN\n",
    "\n",
    "    Args:\n",
    "        x: The previous layer in the model\n",
    "        s: The stack index i.e. which stack in the overall TCN\n",
    "        i: The dilation power of 2 we are using for this residual block\n",
    "        activation: The name of the type of activation to use\n",
    "        nb_filters: The number of convolutional filters to use in this block\n",
    "        kernel_size: The size of the convolutional kernel\n",
    "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "    Returns:\n",
    "        A tuple where the first element is the residual model layer, and the second\n",
    "        is the skip connection.\n",
    "    \"\"\"\n",
    "\n",
    "    original_x = x\n",
    "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding=padding,\n",
    "                  name=name + '_d_%s_conv_%d_tanh_s%d' % (padding, i, s))(x)\n",
    "    if activation == 'norm_relu':\n",
    "        x = Activation('relu')(conv)\n",
    "        x = Lambda(channel_normalization)(x)\n",
    "    elif activation == 'wavenet':\n",
    "        x = wave_net_activation(conv)\n",
    "    else:\n",
    "        x = Activation(activation)(conv)\n",
    "\n",
    "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
    "\n",
    "    # 1x1 conv.\n",
    "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
    "    res_x = tensorflow.keras.layers.add([original_x, x])\n",
    "    return res_x, x\n",
    "\n",
    "\n",
    "def process_dilations(dilations):\n",
    "    def is_power_of_two(num):\n",
    "        return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
    "        return new_dilations\n",
    "\n",
    "\n",
    "class TCN:\n",
    "    \"\"\"Creates a TCN layer.\n",
    "\n",
    "        Input shape:\n",
    "            A tensor of shape (batch_size, timesteps, input_dim).\n",
    "\n",
    "        Args:\n",
    "            nb_filters: The number of filters to use in the convolutional layers.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            activation: The activations to use (norm_relu, wavenet, relu...).\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=[1, 2, 4, 8, 16],\n",
    "                 activation='norm_relu',\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=True,\n",
    "                 name='tcn'):\n",
    "        self.name = name\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.activation = activation\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            print('An interface change occurred after the version 2.1.2.')\n",
    "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
    "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
    "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
    "            raise Exception()\n",
    "\n",
    "    def __call__(self, inputs, Activation_of_skip_connections='selu'):\n",
    "        x = inputs\n",
    "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
    "        skip_connections = []\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i in self.dilations:\n",
    "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
    "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
    "                skip_connections.append(skip_out)\n",
    "        if self.use_skip_connections:\n",
    "            x = tensorflow.keras.layers.add(skip_connections)\n",
    "        x = Activation(Activation_of_skip_connections)(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            output_slice_index = -1\n",
    "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compiled_tcn(num_feat,  # type: int\n",
    "                 num_classes,  # type: int\n",
    "                 nb_filters,  # type: int\n",
    "                 kernel_size,  # type: int\n",
    "                 dilations,  # type: List[int]\n",
    "                 nb_stacks,  # type: int\n",
    "                 max_len,  # type: int\n",
    "                 activation='norm_relu',  # type: str\n",
    "                 padding='causal',  # type: str\n",
    "                 use_skip_connections=True,  # type: bool\n",
    "                 return_sequences=True,\n",
    "                 regression=False,  # type: bool\n",
    "                 dropout_rate=0.05,  # type: float\n",
    "                 name='tcn'  # type: str\n",
    "                 ):\n",
    "    # type: (...) -> tensorflow.keras.Model\n",
    "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
    "\n",
    "    Args:\n",
    "        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n",
    "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
    "        nb_filters: The number of filters to use in the convolutional layers.\n",
    "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "        nb_stacks : The number of stacks of residual blocks to use.\n",
    "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
    "        activation: The activations to use.\n",
    "        padding: The padding to use in the convolutional layers.\n",
    "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "        regression: Whether the output should be continuous or discrete.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "    Returns:\n",
    "        A compiled keras TCN.\n",
    "    \"\"\"\n",
    "\n",
    "    dilations = process_dilations(dilations)\n",
    "\n",
    "    input_layer = Input(shape=(max_len, num_feat))\n",
    "\n",
    "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, activation,\n",
    "            padding, use_skip_connections, dropout_rate, return_sequences, name)(input_layer)\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "    if not regression:\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        output_layer = x\n",
    "        print(f'model.x = {input_layer.shape}')\n",
    "        print(f'model.y = {output_layer.shape}')\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO To remove later.\n",
    "        def accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            if K.ndim(y_true) == K.ndim(y_pred):\n",
    "                y_true = K.squeeze(y_true, -1)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
    "        model.compile(adam, loss='sparse_categorical_crossentropy', metrics=[accuracy])\n",
    "        print('Adam with norm clipping.')\n",
    "    else:\n",
    "        # regression\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('linear')(x)\n",
    "        output_layer = x\n",
    "        print(f'model.x = {input_layer.shape}')\n",
    "        print(f'model.y = {output_layer.shape}')\n",
    "        model = Model(input_layer, output_layer)\n",
    "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
    "        model.compile(adam, loss=tf_stock_loss_7(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_stock_loss_5(vex_sign,cave_sign):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred_previous= tf.roll(y_pred, shift=-1, axis=0)\n",
    "        rising = K.less(y_pred_previous,y_pred)\n",
    "        falling = K.greater(y_pred_previous ,y_pred)\n",
    "        convex= K.equal(vex_sign,tf.ones(shape = tf.shape(vex_sign)))\n",
    "        concave= K.equal(cave_sign,tf.ones(shape = tf.shape(cave_sign)))\n",
    "        losses = tf.keras.backend.cast(tf.logical_and(convex,rising),dtype='float32')*penalty* mean_squared_error(y_true, y_pred)+\\\n",
    "                 tf.keras.backend.cast(tf.logical_and(concave,falling),dtype='float32')*penalty*mean_squared_error(y_true, y_pred)+\\\n",
    "                 mean_squared_error(y_true, y_pred)\n",
    "        #losses = K.all(K.stack([convex, rising], axis=0), axis=0)*penalty* mean_squared_error(y_true, y_pred)+mean_squared_error(y_true, y_pred)\n",
    "        \"\"\"K.all(K.stack([concave, falling], axis=0), axis=0)*penalty** mean_squared_error(y_true, y_pred)+\"\"\"\n",
    "        return losses\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_stock_loss_6(vex_sign,cave_sign):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred_tobemodified0= tf.reshape(y_pred, [-1])\n",
    "        y_pred_tobemodified1= tf.roll(y_pred_tobemodified0, shift=1, axis=0)\n",
    "        y_pred_previous= tf.reshape(y_pred_tobemodified1,tf.shape(y_pred))\n",
    "        \n",
    "        rising = K.less(y_pred_previous,y_pred)\n",
    "        falling = K.greater(y_pred_previous ,y_pred)\n",
    "        y_pred_falling= K.equal(vex_sign,tf.ones(shape = tf.shape(vex_sign)))\n",
    "        y_pred_rising= K.equal(cave_sign,tf.ones(shape = tf.shape(cave_sign)))\n",
    "        losses = tf.keras.backend.cast(tf.logical_and(y_pred_falling,rising),dtype='float32')*penalty* mean_squared_error(y_true, y_pred)+\\\n",
    "                 tf.keras.backend.cast(tf.logical_and(y_pred_rising,falling),dtype='float32')*penalty*mean_squared_error(y_true, y_pred)+\\\n",
    "                 mean_squared_error(y_true, y_pred)       \n",
    "        return losses\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_stock_loss_7(vex_sign,cave_sign):\n",
    "    def loss(y_true, y_pred):\n",
    "        \n",
    "        y_pred_tobemodified0= tf.reshape(y_pred, [-1])\n",
    "        y_pred_tobemodified1= tf.roll(y_pred_tobemodified0, shift=1, axis=0)\n",
    "        y_pred_previous= tf.reshape(y_pred_tobemodified1,tf.shape(y_pred))\n",
    "        \n",
    "        y_pred_rising = K.less(y_pred_previous,y_pred)\n",
    "        y_pred_falling = K.greater(y_pred_previous ,y_pred)    \n",
    "\n",
    "        y_true_tobemodified0= tf.reshape(y_true, [-1])\n",
    "        y_true_tobemodified1= tf.roll(y_true_tobemodified0, shift=1, axis=0)\n",
    "        y_true_previous= tf.reshape(y_true_tobemodified1,tf.shape(y_true))\n",
    "                                                                  \n",
    "        y_true_rising = K.less(y_true_previous,y_true)\n",
    "        y_true_falling = K.greater(y_true_previous ,y_true)\n",
    "\n",
    "        losses = tf.keras.backend.cast(tf.logical_and(y_true_falling,y_pred_rising),dtype='float32')*penalty* mean_squared_error(y_true, y_pred)+\\\n",
    "                 tf.keras.backend.cast(tf.logical_and(y_true_rising,y_pred_falling),dtype='float32')*penalty*mean_squared_error(y_true, y_pred)+\\\n",
    "                 mean_squared_error(y_true, y_pred)       \n",
    "        return losses\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_stock_loss_8(vex_sign,cave_sign):\n",
    "    def loss(y_true, y_pred):\n",
    "        indices = tf.constant([[0]])\n",
    "        updates = tf.constant([False])\n",
    "        \n",
    "        y_pred_tobemodified0= tf.reshape(y_pred, [-1])\n",
    "        y_pred_tobemodified1= tf.roll(y_pred_tobemodified0, shift=1, axis=0)\n",
    "        y_pred_previous= tf.reshape(y_pred_tobemodified1,tf.shape(y_pred))\n",
    "        \n",
    "        y_pred_rising0 = K.less(y_pred_previous,y_pred)\n",
    "        y_pred_falling0 = K.greater(y_pred_previous ,y_pred)    \n",
    "        y_pred_rising = tf.tensor_scatter_update(y_pred_rising0, indices, updates)\n",
    "        y_pred_falling = tf.tensor_scatter_update(y_pred_falling0, indices, updates)\n",
    "        \n",
    "        y_true_tobemodified0= tf.reshape(y_true, [-1])\n",
    "        y_true_tobemodified1= tf.roll(y_true_tobemodified0, shift=1, axis=0)\n",
    "        y_true_previous= tf.reshape(y_true_tobemodified1,tf.shape(y_true))\n",
    "                                                                  \n",
    "        y_true_rising0 = K.less(y_true_previous,y_true)\n",
    "        y_true_falling0 = K.greater(y_true_previous ,y_true)\n",
    "        y_true_rising = tf.tensor_scatter_update(y_true_rising0, indices, updates)\n",
    "        y_true_falling = tf.tensor_scatter_update(y_true_falling0, indices, updates)\n",
    "        \n",
    "        losses = tf.keras.backend.cast(tf.logical_and(y_true_falling,y_pred_rising),dtype='float32')*penalty* mean_squared_error(y_true, y_pred)+\\\n",
    "                 tf.keras.backend.cast(tf.logical_and(y_true_rising,y_pred_falling),dtype='float32')*penalty*mean_squared_error(y_true, y_pred)+\\\n",
    "                 mean_squared_error(y_true, y_pred)       \n",
    "        return losses\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_r2_score(file_name):\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "    file_end_with = '_lr0001_dila16.csv'\n",
    "    #above need to be adjust\n",
    "    df_for_eval= pd.read_csv(Target_DirPath + file_name + file_end_with)\n",
    "    df_for_eval['Date'] = pd.to_datetime(df_for_eval[\"Date\"])\n",
    "    df_for_eval = df_for_eval.set_index([\"Date\"], drop=True)\n",
    "\n",
    "    y_pred_for_eval = df_for_eval[\"Close\"]\n",
    "    print(y_pred_for_eval.shape)\n",
    "    ## create empty table with label fields\n",
    "    y_pred_for_eval_data_like = np.zeros(shape=(len(y_pred_for_eval), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_pred_for_eval_data_like[:,0] = y_pred_for_eval[:]\n",
    "    ## transform and then select the right field\n",
    "    y_pred_for_eval_data = sc.transform(y_pred_for_eval_data_like)[:,0]\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_for_eval_data)\n",
    "    print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "    y_test_diff = np.diff(y_test) #y_test[i]與y_test[i-1]差異\n",
    "    y_pred_for_eval_data_diff = np.diff(y_pred_for_eval_data)#y_pred[i]與y_pred[i-1]差異\n",
    "    rsquare_product = y_test_diff*y_pred_for_eval_data_diff #兩者相乘\n",
    "    def return_same_sign_bool(d):\n",
    "        d = np.array(d)\n",
    "        return np.where(d > 0, 1, 0)\n",
    "    rsquare_product_bool = return_same_sign_bool(rsquare_product) #如果兩者相乘為正數回傳1，非正數回傳0\n",
    "    print(\"The Custom  sign score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))) \n",
    "               #計算y_test及y_pred變動同向機率\n",
    "    print(\"The Custom  R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])/2 \n",
    "                      + ((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))/2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe0.00001_tcn9'\n",
    "penalty=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe0.00001_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tcn9~11 change filter size pe0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe0.0001_tcn9'\n",
    "penalty=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe0.0001_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tcn9~11 change filter size pe0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe0.001_tcn9'\n",
    "penalty=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe0.001_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tcn9~11 change filter size pe0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe0.01_tcn9'\n",
    "penalty=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe0.01_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe0.1_tcn9'\n",
    "penalty=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe0.1_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe1_tcn9'\n",
    "penalty=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe1_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe10_tcn9'\n",
    "penalty=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe10_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcn9\n",
    "file_name='NYA_de5_clf5_pe100_tcn9'\n",
    "penalty=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "input_tensor, vex_sign, cave_sign = Input(shape=(1,X_train.shape[1],)), Input(shape=(None,)), Input(shape=(None,))\n",
    "output = TCN(nb_filters=128, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "        activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "        dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model([input_tensor,vex_sign,cave_sign], output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=tf_stock_loss_8(vex_sign=vex_sign,cave_sign=cave_sign))\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model = model.fit(x=[X_tr_t,vex_sign_train,cave_sign_train], y=y_train, epochs=600, \n",
    "                                    batch_size=16, verbose=1,\n",
    "                                    validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_architecture(model=model, file_name=file_name)\n",
    "y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  vex_sign=vex_sign_test,cave_sign=cave_sign_test,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "             file_name=file_name,test_date=test_date)\n",
    "save_train_csv(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iter_number in range(9,10):\n",
    "    iter_number = str(iter_number)\n",
    "    file_name='NYA_de5_clf5_pe100_tcn'+iter_number\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env002",
   "language": "python",
   "name": "env002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
