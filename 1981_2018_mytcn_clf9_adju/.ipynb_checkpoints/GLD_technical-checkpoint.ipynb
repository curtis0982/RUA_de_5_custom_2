{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nextera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-644e6d12f0cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdensenet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minception_resnet_v2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\densenet\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseNet169\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\applications\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseNet121\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseNet169\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseNet201\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\applications\\densenet.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimagenet_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_eager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_gpu_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Bojun_Zhang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\utils\\multi_gpu_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Model'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/python_code/data/stock/nextera'\n",
    "fname = os.path.join(data_dir, 'NEE_1981_2017_without_volume.csv')\n",
    "df = pd.read_csv(fname)\n",
    "df['observation_date'] = pd.to_datetime(df[\"observation_date\"])\n",
    "df_idx = df.set_index([\"observation_date\"], drop=True)\n",
    "df_idx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把順序調換\n",
    "df_idx = df_idx.sort_index(axis=0, ascending=False)\n",
    "df_idx = df_idx.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_idx\n",
    "data.plot(y='Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.index.values[-1] - data.index.values[0]\n",
    "days = diff.astype('timedelta64[D]')\n",
    "days = days / np.timedelta64(1, 'D')\n",
    "years = int(days/365)\n",
    "print(\"total data days:\",days)\n",
    "print(\"Total data: %d years\"%years)\n",
    "print(\"80 percent data = 1980 to %d\"%(1971 + int(0.8*years)))\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#切割訓練與測試資料\n",
    "split_date = pd.Timestamp('01-01-2001')\n",
    "\n",
    "train = data.loc[:split_date]\n",
    "test = data.loc[split_date:]\n",
    "test_date = test.index\n",
    "test_date = pd.to_datetime(test_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料正規化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without sc\n",
    "#train_sc = train\n",
    "#test_sc = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df = pd.DataFrame(train_sc,index=train.index,columns=train.columns)\n",
    "test_sc_df = pd.DataFrame(test_sc,index=test.index,columns=test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(-1,0):\n",
    "    train_sc_df['Y_{}'.format(s)] = train_sc_df['Close'].shift(s)\n",
    "    test_sc_df['Y_{}'.format(s)] = test_sc_df['Close'].shift(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = train_sc_df.dropna().drop('Y', axis=1)\n",
    "X_train = train_sc_df.dropna().drop('Y_-1', axis=1)\n",
    "y_train = train_sc_df.dropna()['Y_-1']\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "X_train = X_train[:-1]\n",
    "y_train = y_train[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_sc_df.dropna().drop('Y_-1', axis=1)\n",
    "y_test = test_sc_df.dropna().dropna()['Y_-1']\n",
    "\n",
    "X_test = X_test.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "\n",
    "X_test = X_test[:-1]\n",
    "y_test = y_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train size: (%d x %d)'%(X_train.shape[0], X_train.shape[1]))\n",
    "print('Test size: (%d x %d)'%(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Add\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import CuDNNLSTM, LSTM\n",
    "from keras import Input,layers\n",
    "from keras.models import Model\n",
    "\n",
    "def adj_r2_score(r2, n, k):\n",
    "    return 1-((1-r2)*((n-1)/(n-k-1)))\n",
    "\n",
    "val_split_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用1層網路，啟動函數為relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(X_test.shape[1],), activation='relu', \n",
    "                kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_1l_50n.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_test.shape[1],), activation='relu',\n",
    "                kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_1l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape=(X_test.shape[1],), activation='relu', \n",
    "                kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_1l_200n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 試著用2層網路，啟動函數換為relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(X_test.shape[1],), activation='relu'))\n",
    "model.add(Dense(50, input_shape=(X_test.shape[1],), activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "history= model.fit(X_train, y_train, batch_size=64,\n",
    "                   validation_split=val_split_ratio,\n",
    "          epochs=200, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_2l_50n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_test.shape[1],), activation='relu', kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(100, input_shape=(X_test.shape[1],), activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_2l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape=(X_test.shape[1],), activation='relu', kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(200, input_shape=(X_test.shape[1],), activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_2l_200n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import Input,layers\n",
    "from keras.models import Model\n",
    "input_tensor = Input(shape=(X_test.shape[1],))\n",
    "x = Dense(50, kernel_initializer='lecun_uniform' ,activation='relu')(input_tensor)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "y = Dense(50, activation='relu')(input_tensor)\n",
    "output=layers.concatenate(\n",
    "[x,y], axis=-1)\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_res_2l_50n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import Input,layers\n",
    "from keras.models import Model\n",
    "input_tensor = Input(shape=(X_test.shape[1],))\n",
    "x = Dense(100, kernel_initializer='lecun_uniform' ,activation='relu')(input_tensor)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "y = Dense(100, activation='relu')(input_tensor)\n",
    "output=layers.concatenate(\n",
    "[x,y], axis=-1)\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_res_2l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import Input,layers\n",
    "from keras.models import Model\n",
    "input_tensor = Input(shape=(X_test.shape[1],))\n",
    "x = Dense(200, kernel_initializer='lecun_uniform' ,activation='relu')(input_tensor)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "y = Dense(200, activation='relu')(input_tensor)\n",
    "output=layers.concatenate(\n",
    "[x,y], axis=-1)\n",
    "output_tensor = Dense(1)(output)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history= model.fit(X_train, y_train, batch_size=64,validation_split= val_split_ratio,\n",
    "          epochs=200, verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_shift = np.roll(y_pred,1)\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='pred')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_res_2l_200n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "from keras import Input,layers\n",
    "from keras.models import Model\n",
    "input_tensor = Input(shape=(1,X_train.shape[1],))\n",
    "z=CuDNNLSTM(50,  kernel_initializer='lecun_uniform', return_sequences=True)(input_tensor)\n",
    "z=CuDNNLSTM(50,  return_sequences=False)(z)\n",
    "t=CuDNNLSTM(50,  kernel_initializer='lecun_uniform', return_sequences=False)(input_tensor)\n",
    "\"\"\"x = Dense(400, activation='relu')(z)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "y = Dense(400, activation='relu')(z)\"\"\"\n",
    "\n",
    "output=layers.concatenate([z,t], axis=-1)\n",
    "output = Dense(50, activation='relu')(output)\n",
    "output_tensor = Dense(1)(output)\n",
    "\n",
    "\n",
    "model_lstm = Model(input_tensor, output_tensor)\n",
    "model_lstm.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,\n",
    "                                    validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, batch_size=64, verbose=1, shuffle=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_lsres_lsres_2l_50n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "from keras import Input,layers\n",
    "from keras.models import Model\n",
    "input_tensor = Input(shape=(1,X_train.shape[1],))\n",
    "z=CuDNNLSTM(100,  kernel_initializer='lecun_uniform', return_sequences=True)(input_tensor)\n",
    "z=CuDNNLSTM(100,  return_sequences=False)(z)\n",
    "t=CuDNNLSTM(100,  kernel_initializer='lecun_uniform', return_sequences=False)(input_tensor)\n",
    "\"\"\"x = Dense(400, activation='relu')(z)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "y = Dense(400, activation='relu')(z)\"\"\"\n",
    "\n",
    "output=layers.concatenate([z,t], axis=-1)\n",
    "output = Dense(100, activation='relu')(output)\n",
    "output_tensor = Dense(1)(output)\n",
    "\n",
    "\n",
    "model_lstm = Model(input_tensor, output_tensor)\n",
    "model_lstm.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,\n",
    "                                    validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, batch_size=64, verbose=1, shuffle=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_lsres_lsres_2l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "from keras import Input,layers\n",
    "from keras.models import Model\n",
    "input_tensor = Input(shape=(1,X_train.shape[1],))\n",
    "z=CuDNNLSTM(200,  kernel_initializer='lecun_uniform', return_sequences=True)(input_tensor)\n",
    "z=CuDNNLSTM(200,  return_sequences=False)(z)\n",
    "t=CuDNNLSTM(200,  kernel_initializer='lecun_uniform', return_sequences=False)(input_tensor)\n",
    "\"\"\"x = Dense(400, activation='relu')(z)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "y = Dense(400, activation='relu')(z)\"\"\"\n",
    "\n",
    "output=layers.concatenate([z,t], axis=-1)\n",
    "output = Dense(200, activation='relu')(output)\n",
    "output_tensor = Dense(1)(output)\n",
    "\n",
    "\n",
    "model_lstm = Model(input_tensor, output_tensor)\n",
    "model_lstm.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,\n",
    "                                    validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, batch_size=64, verbose=1, shuffle=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_lsres_lsres_2l_200n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下來使用LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 增加神經元為50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(50, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform', return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_1l_50n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(100, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform', return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_1l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(200, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform', return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "\"\"\"history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\"\"\"\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_1l_200n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 雙層LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tst_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(50, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform', return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(50, return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_2l_50n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(100, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform', return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(100, return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_2l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(200, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform', return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(200, return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_2l_200n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(50, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(50, kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(50, kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "\"\"\"history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\"\"\"\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False)\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_3l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(100, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(100, kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(100, kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n",
    "\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_3l_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNLSTM(200, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(200, kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=True))\n",
    "model_lstm.add(CuDNNLSTM(200, kernel_initializer='lecun_uniform',\n",
    "                         return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,validation_split= val_split_ratio,\n",
    "                                    shuffle=False,\n",
    "                                    callbacks=[early_stop])\n",
    "\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_3l_200n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm+res 1ls_2lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "input_tensor = Input(shape=(1,X_train.shape[1],))\n",
    "z=CuDNNLSTM(50,   return_sequences=False)(input_tensor)\n",
    "x = Dense(50, activation='relu')(z)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "y = Dense(50, activation='relu')(z)\n",
    "\n",
    "output=layers.concatenate([x,y], axis=-1)\n",
    "#output =Dense(100, activation='relu')(output)\n",
    "output_tensor = Dense(1)(output)\n",
    "\n",
    "\n",
    "model_lstm = Model(input_tensor, output_tensor)\n",
    "model_lstm.compile(optimizer=RMSprop(lr=0.001), loss='mean_squared_error')\n",
    "#model_lstm.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,\n",
    "                                    validation_split= val_split_ratio,\n",
    "                                    shuffle=False)\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_res_ls1l_res2l_50n.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "input_tensor = Input(shape=(1,X_train.shape[1],))\n",
    "z=CuDNNLSTM(100,   return_sequences=False)(input_tensor)\n",
    "x = Dense(100, activation='relu')(z)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "y = Dense(100, activation='relu')(z)\n",
    "\n",
    "output=layers.concatenate([x,y], axis=-1)\n",
    "#output =Dense(100, activation='relu')(output)\n",
    "output_tensor = Dense(1)(output)\n",
    "\n",
    "\n",
    "model_lstm = Model(input_tensor, output_tensor)\n",
    "model_lstm.compile(optimizer=RMSprop(lr=0.001), loss='mean_squared_error')\n",
    "#model_lstm.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,\n",
    "                                    validation_split= val_split_ratio,\n",
    "                                    shuffle=False)\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_res_ls1l_res2l_100n.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "input_tensor = Input(shape=(1,X_train.shape[1],))\n",
    "z=CuDNNLSTM(100,   return_sequences=False)(input_tensor)\n",
    "x = Dense(100, activation='relu')(z)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "y = Dense(100, activation='relu')(z)\n",
    "\n",
    "output=layers.concatenate([x,y], axis=-1)\n",
    "#output =Dense(100, activation='relu')(output)\n",
    "output_tensor = Dense(1)(output)\n",
    "\n",
    "\n",
    "model_lstm = Model(input_tensor, output_tensor)\n",
    "model_lstm.compile(optimizer=RMSprop(lr=0.001), loss='mean_squared_error')\n",
    "#model_lstm.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,\n",
    "                                    validation_split= val_split_ratio,\n",
    "                                    shuffle=False)\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_1ls_2res_100n.csv.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import keras.backend as K\n",
    "\n",
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "input_tensor = Input(shape=(1,X_train.shape[1],))\n",
    "z=CuDNNLSTM(200,   return_sequences=False)(input_tensor)\n",
    "x = Dense(200, activation='relu')(z)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "y = Dense(200, activation='relu')(z)\n",
    "\n",
    "output=layers.concatenate([x,y], axis=-1)\n",
    "#output =Dense(100, activation='relu')(output)\n",
    "output_tensor = Dense(1)(output)\n",
    "\n",
    "\n",
    "model_lstm = Model(input_tensor, output_tensor)\n",
    "model_lstm.compile(optimizer=RMSprop(lr=0.001), loss='mean_squared_error')\n",
    "#model_lstm.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, \n",
    "                                    batch_size=64, verbose=1,\n",
    "                                    validation_split= val_split_ratio,\n",
    "                                    shuffle=False)\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_shift = np.roll(y_pred_test_lstm,1)\n",
    "y_test1 = y_test[1:]\n",
    "y_pred_shift = y_pred_shift[1:]\n",
    "print('R-Squared: %f'%(r2_score(y_test1, y_pred_shift)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history_model_lstm.history.keys())\n",
    "\n",
    "loss = history_model_lstm.history['loss']\n",
    "val_loss = history_model_lstm.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把價格縮放解除\n",
    "## create empty table with 12 fields\n",
    "y_pred_data_like = np.zeros(shape=(len(y_pred_test_lstm), X_train.shape[1]))\n",
    "## put the predicted values in the right field\n",
    "y_pred_data_like[:,0] = y_pred_test_lstm[:,0]\n",
    "## inverse transform and then select the right field\n",
    "y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "#檢查測試資料的維度\n",
    "yd_size = X_test.shape[0]\n",
    "\n",
    "# 把價格轉換維度\n",
    "yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "# 刪除頭一天(從這天開始，所以沒價格)\n",
    "test_date_trim = np.delete(test_date, 0)\n",
    "\n",
    "# 放在MC要推移一天，所以要刪除最後一天\n",
    "test_date_trim = np.delete(test_date_trim, test_date_trim.size - 1)\n",
    "\n",
    "# 製作CSV\n",
    "AnalysisResult = pd.DataFrame()\n",
    "Date = pd.Series(test_date_trim)\n",
    "Price = pd.Series(yd)\n",
    "Date.name = 'Date'\n",
    "Price.name = 'Price'\n",
    "\n",
    "# 因為放在MC要開高低收，所以複製收盤填入\n",
    "Open = Price.copy()\n",
    "High = Price.copy()\n",
    "Low = Price.copy()\n",
    "Open.name = 'Open'\n",
    "High.name = 'High'\n",
    "Low.name = 'Low'\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Price], axis=1)\n",
    "\n",
    "AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "# 輸出CSV檔案\n",
    "import os\n",
    "Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "\n",
    "if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "AnalysisResult.to_csv(Target_DirPath + '//nextera_ls_res_ls1l_res2l_200n.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 單層GRU,act=relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNGRU\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(CuDNNGRU(6, input_shape=(1, X_train.shape[1]), kernel_initializer='lecun_uniform', return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.summary()\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=200, batch_size=64, verbose=1, shuffle=False)\n",
    "\n",
    "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
    "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
    "r2_train = r2_score(y_train, y_train_pred_lstm)\n",
    "print(\"The Adjusted R2 score on the Train set is:\\t{:0.3f}\\n\".format(adj_r2_score(r2_train, X_train.shape[0], X_train.shape[1])))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n",
    "r2_test = r2_score(y_test, y_pred_test_lstm)\n",
    "print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\".format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred_test_lstm, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('TWD_Scaled')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best model yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_shape=(X_test.shape[1],), activation='relu', kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(50, input_shape=(X_test.shape[1],), activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=200, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('more_label_lstm_nextera_with_economics.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('more_label_lstm_nextera_with_economics.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.plot(y_test)\n",
    "plt.plot(y_pred)\n",
    "print('R-Squared: %f'%(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
