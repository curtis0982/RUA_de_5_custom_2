{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f83c6d50081b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[0mparent_package_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     child_package_str=(\n\u001b[1;32m--> 637\u001b[1;33m         'tensorflow_estimator.python.estimator.api._v1.estimator'))\n\u001b[0m\u001b[0;32m    638\u001b[0m _component_api_helper.package_hook(\n\u001b[0;32m    639\u001b[0m     \u001b[0mparent_package_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow\\python\\tools\\component_api_helper.py\u001b[0m in \u001b[0;36mpackage_hook\u001b[1;34m(parent_package_str, child_package_str, error_msg)\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[0mparent_pkg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent_package_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mchild_pkg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_package_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merror_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0m_names_with_underscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_s\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_s\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\_api\\v2\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbaseline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaselineClassifierV2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mBaselineClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\_api\\v2\\estimator\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdnn_logit_fn_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearSDCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_logit_fn_builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msession_run_hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode_keys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\inputs.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\numpy_io.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueues\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeeding_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m   \u001b[0mHAS_PANDAS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     from pandas._libs import (hashtable as _hashtable,\n\u001b[0m\u001b[0;32m     27\u001b[0m                              \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                              tslib as _tslib)\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      5\u001b[0m     iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime, Period)\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz_convert_single\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\conversion.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\timedeltas.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.timedeltas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\offsets.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.offsets\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\ccalendar.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.ccalendar\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.TimeRE.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.TimeRE.__seqToRE\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\pytz\\lazy.py\u001b[0m in \u001b[0;36m_lazy\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_iter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_props\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                             \u001b[0mdelattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLazyList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1080\u001b[0m  'Zulu']\n\u001b[0;32m   1081\u001b[0m all_timezones = LazyList(\n\u001b[1;32m-> 1082\u001b[1;33m         tz for tz in all_timezones if resource_exists(tz))\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[0mall_timezones_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLazySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_timezones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mresource_exists\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;34m\"\"\"Return true if the given resource exists\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mopen_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Windows\\system32\\env002\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mopen_resource\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresource_stream\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresource_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zoneinfo/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__file__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work: model  9 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot_ng\n",
    "import timeit\n",
    "%matplotlib inline\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_imagepath ='images/loss/'\n",
    "predict_imagepath ='images/predict/'\n",
    "losspath = 'csv/loss/'\n",
    "if (not (os.path.exists(losspath))):\n",
    "        os.makedirs(losspath)\n",
    "model_dirpath = 'h5/'\n",
    "file_name='file_name'\n",
    "\n",
    "#for func initiate\n",
    "history_model='history_model'\n",
    "input_tensor='input_tensor'\n",
    "y_pred='y_pred'\n",
    "Target_DirPath='Target_DirPath'\n",
    "test_date_trim='test_date_trim'\n",
    "train_date_trim='train_date_trim'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Add,Reshape,Lambda\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping,CSVLogger\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import Input,layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.utils import plot_model\n",
    "K.clear_session()\n",
    "#from tcn import compiled_tcn,TCN\n",
    "\n",
    "def adj_r2_score(r2, n, k):\n",
    "    return 1-((1-r2)*((n-1)/(n-k-1)))\n",
    "\n",
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "val_split_ratio = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_architecture(model, file_name):\n",
    "    file_path = 'images/model/{}.png'.format(file_name)\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    plot_model(model, to_file=file_path, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel(model_dirpath=model_dirpath,file_name=file_name):\n",
    "    model = load_model(model_dirpath + file_name + '.h5')\n",
    "    return model\n",
    "def loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    loss = history_model.history['loss']\n",
    "    val_loss = history_model.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    if (not (os.path.exists(loss_imagepath))):\n",
    "                os.makedirs(loss_imagepath)\n",
    "    plt.savefig(loss_imagepath +  file_name +'_loss.png')\n",
    "    plt.show()    \n",
    "\n",
    "def CSV(losspath=losspath, file_name=file_name):\n",
    "    csv_logger = CSVLogger(losspath + file_name + '_log.csv')\n",
    "    return csv_logger\n",
    "def predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name):\n",
    "    from sklearn.metrics import r2_score\n",
    "    y_pred = model.predict([X_tst_t])\n",
    "    #y_pred是三天前就知道，所以往前移三格\n",
    "    y_test_pic = y_test[:]\n",
    "    y_pred_pic = y_pred[delay:]\n",
    "    y_test_rsquare = y_test[:]\n",
    "    plt.plot(y_test_pic, label='True')\n",
    "    plt.plot(y_pred_pic, label='pred')\n",
    "    plt.xlabel('Observation')\n",
    "    plt.ylabel('Scaled_Value')\n",
    "    plt.legend()\n",
    "    print(\"y_pred.shape:\",y_pred.shape)\n",
    "    print(\"y_test_rsquare.shape:\",y_test_rsquare.shape)\n",
    "    r2_test = r2_score(y_test_rsquare, y_pred)\n",
    "    print('R-Squared: %f'%(r2_test))\n",
    "    print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "          .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "    if (not (os.path.exists(predict_imagepath))):\n",
    "            os.makedirs(predict_imagepath)\n",
    "    plt.savefig(predict_imagepath +  file_name +'_loss.png')\n",
    "    plt.show()    \n",
    "    return y_pred\n",
    "def save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\\\n",
    "             file_name=file_name,test_date=test_date):\n",
    "    # 把價格縮放解除\n",
    "    ## create empty table with label fields\n",
    "    y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_pred_data_like[:,0] = y_pred[:,0]\n",
    "    ## inverse transform and then select the right field\n",
    "    y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "    #檢查測試資料的維度\n",
    "    yd_size = X_test.shape[0]\n",
    "\n",
    "    # 把價格轉換維度\n",
    "    yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "    #把最後X天刪除(預測X天後)\n",
    "    test_date_trim = np.delete(test_date, np.s_[-delay:])\n",
    "\n",
    "    # 製作CSV\n",
    "    AnalysisResult = pd.DataFrame()\n",
    "    Date = pd.Series(test_date_trim)\n",
    "    Close = pd.Series(yd)\n",
    "    Date.name = 'Date'\n",
    "    Close.name = 'Close'\n",
    "\n",
    "    # 因為放在MC要開高低收，所以複製收盤填入\n",
    "    Open = Close.copy()\n",
    "    High = Close.copy()\n",
    "    Low = Close.copy()\n",
    "    Open.name = 'Open'\n",
    "    High.name = 'High'\n",
    "    Low.name = 'Low'\n",
    "\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Close], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "    # 輸出CSV檔案\n",
    "    import os\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "    if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "    AnalysisResult.to_csv(Target_DirPath + file_name+'.csv', mode='w', header=True, index=False)\n",
    "    \n",
    "def save_train_csv(file_name=file_name):\n",
    "    y_train_pred = model.predict([X_tr_t])\n",
    "    # 把價格縮放解除\n",
    "    ## create empty table with label fields\n",
    "    y_train_pred_data_like = np.zeros(shape=(len(y_train_pred), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_train_pred_data_like[:,0] = y_train_pred[:,0]\n",
    "    ## inverse transform and then select the right field\n",
    "    y_train_pred_data = sc.inverse_transform(y_train_pred_data_like)[:,0]\n",
    "\n",
    "    #檢查測試資料的維度\n",
    "    yd_size = X_train.shape[0]\n",
    "\n",
    "    # 把價格轉換維度\n",
    "    yd = y_train_pred_data.reshape(yd_size,)\n",
    "\n",
    "    #把最後X天刪除(預測X天後)\n",
    "    train_date_trim = np.delete(train_date, np.s_[-delay:])\n",
    "\n",
    "    # 製作CSV\n",
    "    AnalysisResult = pd.DataFrame()\n",
    "    Date = pd.Series(train_date_trim)\n",
    "    Close = pd.Series(yd)\n",
    "    Date.name = 'Date'\n",
    "    Close.name = 'Close'\n",
    "\n",
    "    # 因為放在MC要開高低收，所以複製收盤填入\n",
    "    Open = Close.copy()\n",
    "    High = Close.copy()\n",
    "    Low = Close.copy()\n",
    "    Open.name = 'Open'\n",
    "    High.name = 'High'\n",
    "    Low.name = 'Low'\n",
    "\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Close], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "    # 輸出CSV檔案\n",
    "    import os\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master-train/'\n",
    "    if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "    AnalysisResult.to_csv(Target_DirPath + file_name+'_train.csv', mode='w', header=True, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_of_skip_connections='selu'\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Convolution1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def channel_normalization(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\" Normalize a layer to the maximum activation\n",
    "\n",
    "    This keeps a layers values between zero and one.\n",
    "    It helps with relu's unbounded activation\n",
    "\n",
    "    Args:\n",
    "        x: The layer to normalize\n",
    "\n",
    "    Returns:\n",
    "        A maximal normalized layer\n",
    "    \"\"\"\n",
    "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
    "    out = x / max_values\n",
    "    return out\n",
    "\n",
    "\n",
    "def wave_net_activation(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\"This method defines the activation used for WaveNet\n",
    "\n",
    "    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
    "\n",
    "    Args:\n",
    "        x: The layer we want to apply the activation to\n",
    "\n",
    "    Returns:\n",
    "        A new layer with the wavenet activation applied\n",
    "    \"\"\"\n",
    "    tanh_out = Activation('tanh')(x)\n",
    "    sigm_out = Activation('sigmoid')(x)\n",
    "    return tensorflow.keras.layers.multiply([tanh_out, sigm_out])\n",
    "\n",
    "\n",
    "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
    "    # type: (Layer, int, int, str, int, int, str, float, str) -> Tuple[Layer, Layer]\n",
    "    \"\"\"Defines the residual block for the WaveNet TCN\n",
    "\n",
    "    Args:\n",
    "        x: The previous layer in the model\n",
    "        s: The stack index i.e. which stack in the overall TCN\n",
    "        i: The dilation power of 2 we are using for this residual block\n",
    "        activation: The name of the type of activation to use\n",
    "        nb_filters: The number of convolutional filters to use in this block\n",
    "        kernel_size: The size of the convolutional kernel\n",
    "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "    Returns:\n",
    "        A tuple where the first element is the residual model layer, and the second\n",
    "        is the skip connection.\n",
    "    \"\"\"\n",
    "\n",
    "    original_x = x\n",
    "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding=padding,\n",
    "                  name=name + '_d_%s_conv_%d_tanh_s%d' % (padding, i, s))(x)\n",
    "    if activation == 'norm_relu':\n",
    "        x = Activation('relu')(conv)\n",
    "        x = Lambda(channel_normalization)(x)\n",
    "    elif activation == 'wavenet':\n",
    "        x = wave_net_activation(conv)\n",
    "    else:\n",
    "        x = Activation(activation)(conv)\n",
    "\n",
    "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
    "\n",
    "    # 1x1 conv.\n",
    "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
    "    res_x = tensorflow.keras.layers.add([original_x, x])\n",
    "    return res_x, x\n",
    "\n",
    "\n",
    "def process_dilations(dilations):\n",
    "    def is_power_of_two(num):\n",
    "        return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
    "        return new_dilations\n",
    "\n",
    "\n",
    "class TCN:\n",
    "    \"\"\"Creates a TCN layer.\n",
    "\n",
    "        Input shape:\n",
    "            A tensor of shape (batch_size, timesteps, input_dim).\n",
    "\n",
    "        Args:\n",
    "            nb_filters: The number of filters to use in the convolutional layers.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            activation: The activations to use (norm_relu, wavenet, relu...).\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=[1, 2, 4, 8, 16],\n",
    "                 activation='norm_relu',\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=True,\n",
    "                 name='tcn'):\n",
    "        self.name = name\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.activation = activation\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            print('An interface change occurred after the version 2.1.2.')\n",
    "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
    "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
    "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
    "            raise Exception()\n",
    "\n",
    "    def __call__(self, inputs, Activation_of_skip_connections='selu'):\n",
    "        x = inputs\n",
    "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
    "        skip_connections = []\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i in self.dilations:\n",
    "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
    "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
    "                skip_connections.append(skip_out)\n",
    "        if self.use_skip_connections:\n",
    "            x = tensorflow.keras.layers.add(skip_connections)\n",
    "        x = Activation(Activation_of_skip_connections)(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            output_slice_index = -1\n",
    "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compiled_tcn(num_feat,  # type: int\n",
    "                 num_classes,  # type: int\n",
    "                 nb_filters,  # type: int\n",
    "                 kernel_size,  # type: int\n",
    "                 dilations,  # type: List[int]\n",
    "                 nb_stacks,  # type: int\n",
    "                 max_len,  # type: int\n",
    "                 activation='norm_relu',  # type: str\n",
    "                 padding='causal',  # type: str\n",
    "                 use_skip_connections=True,  # type: bool\n",
    "                 return_sequences=True,\n",
    "                 regression=False,  # type: bool\n",
    "                 dropout_rate=0.05,  # type: float\n",
    "                 name='tcn'  # type: str\n",
    "                 ):\n",
    "    # type: (...) -> tensorflow.keras.Model\n",
    "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
    "\n",
    "    Args:\n",
    "        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n",
    "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
    "        nb_filters: The number of filters to use in the convolutional layers.\n",
    "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "        nb_stacks : The number of stacks of residual blocks to use.\n",
    "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
    "        activation: The activations to use.\n",
    "        padding: The padding to use in the convolutional layers.\n",
    "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "        regression: Whether the output should be continuous or discrete.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "    Returns:\n",
    "        A compiled keras TCN.\n",
    "    \"\"\"\n",
    "\n",
    "    dilations = process_dilations(dilations)\n",
    "\n",
    "    input_layer = Input(shape=(max_len, num_feat))\n",
    "\n",
    "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, activation,\n",
    "            padding, use_skip_connections, dropout_rate, return_sequences, name)(input_layer)\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "    if not regression:\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        output_layer = x\n",
    "        print(f'model.x = {input_layer.shape}')\n",
    "        print(f'model.y = {output_layer.shape}')\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO To remove later.\n",
    "        def accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            if K.ndim(y_true) == K.ndim(y_pred):\n",
    "                y_true = K.squeeze(y_true, -1)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
    "        model.compile(adam, loss='sparse_categorical_crossentropy', metrics=[accuracy])\n",
    "        print('Adam with norm clipping.')\n",
    "    else:\n",
    "        # regression\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('linear')(x)\n",
    "        output_layer = x\n",
    "        print(f'model.x = {input_layer.shape}')\n",
    "        print(f'model.y = {output_layer.shape}')\n",
    "        model = Model(input_layer, output_layer)\n",
    "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
    "        model.compile(adam, loss=tf_stock_loss_9)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_r2_score(file_name):\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "    file_end_with = '.csv'\n",
    "    #above need to be adjust\n",
    "    df_for_eval= pd.read_csv(Target_DirPath + file_name + file_end_with)\n",
    "    df_for_eval['Date'] = pd.to_datetime(df_for_eval[\"Date\"])\n",
    "    df_for_eval = df_for_eval.set_index([\"Date\"], drop=True)\n",
    "\n",
    "    y_pred_for_eval = df_for_eval[\"Close\"]\n",
    "    print(y_pred_for_eval.shape)\n",
    "    ## create empty table with label fields\n",
    "    y_pred_for_eval_data_like = np.zeros(shape=(len(y_pred_for_eval), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_pred_for_eval_data_like[:,0] = y_pred_for_eval[:]\n",
    "    ## transform and then select the right field\n",
    "    y_pred_for_eval_data = sc.transform(y_pred_for_eval_data_like)[:,0]\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_for_eval_data)\n",
    "    print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "    y_test_diff = np.diff(y_test) #y_test[i]與y_test[i-1]差異\n",
    "    y_pred_for_eval_data_diff = np.diff(y_pred_for_eval_data)#y_pred[i]與y_pred[i-1]差異\n",
    "    rsquare_product = y_test_diff*y_pred_for_eval_data_diff #兩者相乘\n",
    "    def return_same_sign_bool(d):\n",
    "        d = np.array(d)\n",
    "        return np.where(d > 0, 1, 0)\n",
    "    rsquare_product_bool = return_same_sign_bool(rsquare_product) #如果兩者相乘為正數回傳1，非正數回傳0\n",
    "    print(\"The Custom  sign score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))) \n",
    "               #計算y_test及y_pred變動同向機率\n",
    "    print(\"The Custom  R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])/2 \n",
    "                      + ((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))/2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:\\python_code\\data\\stock\\^NYA'\n",
    "fname = os.path.join(data_dir, 'NYA 1981 2018 technical_custom_loss4.csv')\n",
    "df = pd.read_csv(fname)\n",
    "df['Date'] = pd.to_datetime(df[\"Date\"])\n",
    "df_idx = df.set_index([\"Date\"], drop=True)\n",
    "df_idx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把順序調換\n",
    "df_idx = df_idx.sort_index(axis=0, ascending=False)\n",
    "df_idx = df_idx.iloc[::-1]\n",
    "\n",
    "data = df_idx\n",
    "print(data.plot(y='Close'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# de10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.index.values[-1] - data.index.values[0]\n",
    "days = diff.astype('timedelta64[D]')\n",
    "days = days / np.timedelta64(1, 'D')\n",
    "years = int(days/365)\n",
    "print(\"total data days:\",days)\n",
    "print(\"Total data: %d years\"%years)\n",
    "print(\"80 percent data = 1981 to %d\"%(1981 + int(0.8*years)))\n",
    "print(diff)\n",
    "delay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#切割訓練與測試資料\n",
    "split_date = pd.Timestamp('01-01-2011')\n",
    "\n",
    "train = data.loc[:split_date]\n",
    "test = data.loc[split_date:]\n",
    "test_date = test.index\n",
    "test_date = pd.to_datetime(test_date)\n",
    "train_date = train.index\n",
    "train_date = pd.to_datetime(train_date)\n",
    "\n",
    "# 資料正規化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)\n",
    "\n",
    "print(\"train_sc.shape:\",train_sc.shape)\n",
    "\n",
    "train_sc_df = pd.DataFrame(train_sc,index=train.index,columns=train.columns)\n",
    "test_sc_df = pd.DataFrame(test_sc,index=test.index,columns=test.columns)\n",
    "\n",
    "for s in range(-delay,-delay+1):\n",
    "    train_sc_df['Y_{}'.format(s)] = train_sc_df['Close'].shift(s)\n",
    "    test_sc_df['Y_{}'.format(s)] = test_sc_df['Close'].shift(s)\n",
    "\n",
    "X_train = train_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_train = train_sc_df.dropna()['Y_-'+str(delay)]\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "\n",
    "X_test = test_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_test = test_sc_df.dropna().dropna()['Y_-'+str(delay)]\n",
    "X_test = X_test.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "\n",
    "print('Train size: (%d x %d)'%(X_train.shape[0], X_train.shape[1]))\n",
    "print('Test size: (%d x %d)'%(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neurons=[32,128,512]\n",
    "for neurons_j in neurons:\n",
    "    file_name='NYA_de'+str(delay)+'_tcn_n'+str(neurons_j)\n",
    "    print(file_name+\":\")\n",
    "    X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "    output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "            activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "            dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "    output_tensor = Dense(1)(output)\n",
    "    model = Model([input_tensor], output_tensor)\n",
    "    model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss='mse')\n",
    "    model.summary()\n",
    "\n",
    "    startTime = timeit.default_timer()\n",
    "\n",
    "    history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                        batch_size=16, verbose=1,\n",
    "                                        validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                        shuffle=False)\n",
    "    elapsedTime = timeit.default_timer() - startTime\n",
    "    print(\"Time taken for the Network to train : \",str(datetime.timedelta(seconds=elapsedTime)))\n",
    "    plot_model_architecture(model=model, file_name=file_name)\n",
    "    y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                      predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "    loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "    save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                 file_name=file_name,test_date=test_date)\n",
    "    save_train_csv(file_name=file_name)\n",
    "\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# de15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.index.values[-1] - data.index.values[0]\n",
    "days = diff.astype('timedelta64[D]')\n",
    "days = days / np.timedelta64(1, 'D')\n",
    "years = int(days/365)\n",
    "print(\"total data days:\",days)\n",
    "print(\"Total data: %d years\"%years)\n",
    "print(\"80 percent data = 1981 to %d\"%(1981 + int(0.8*years)))\n",
    "print(diff)\n",
    "delay = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#切割訓練與測試資料\n",
    "split_date = pd.Timestamp('01-01-2011')\n",
    "\n",
    "train = data.loc[:split_date]\n",
    "test = data.loc[split_date:]\n",
    "test_date = test.index\n",
    "test_date = pd.to_datetime(test_date)\n",
    "train_date = train.index\n",
    "train_date = pd.to_datetime(train_date)\n",
    "\n",
    "# 資料正規化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)\n",
    "\n",
    "print(\"train_sc.shape:\",train_sc.shape)\n",
    "\n",
    "train_sc_df = pd.DataFrame(train_sc,index=train.index,columns=train.columns)\n",
    "test_sc_df = pd.DataFrame(test_sc,index=test.index,columns=test.columns)\n",
    "\n",
    "for s in range(-delay,-delay+1):\n",
    "    train_sc_df['Y_{}'.format(s)] = train_sc_df['Close'].shift(s)\n",
    "    test_sc_df['Y_{}'.format(s)] = test_sc_df['Close'].shift(s)\n",
    "\n",
    "X_train = train_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_train = train_sc_df.dropna()['Y_-'+str(delay)]\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "\n",
    "X_test = test_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_test = test_sc_df.dropna().dropna()['Y_-'+str(delay)]\n",
    "X_test = X_test.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "\n",
    "print('Train size: (%d x %d)'%(X_train.shape[0], X_train.shape[1]))\n",
    "print('Test size: (%d x %d)'%(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neurons=[32,128,512]\n",
    "for neurons_j in neurons:\n",
    "    file_name='NYA_de'+str(delay)+'_tcn_n'+str(neurons_j)\n",
    "    print(file_name+\":\")\n",
    "    X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "    output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "            activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "            dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "    output_tensor = Dense(1)(output)\n",
    "    model = Model([input_tensor], output_tensor)\n",
    "    model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss='mse')\n",
    "    model.summary()\n",
    "\n",
    "    startTime = timeit.default_timer()\n",
    "\n",
    "    history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                        batch_size=16, verbose=1,\n",
    "                                        validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                        shuffle=False)\n",
    "    elapsedTime = timeit.default_timer() - startTime\n",
    "    print(\"Time taken for the Network to train : \",str(datetime.timedelta(seconds=elapsedTime)))\n",
    "    plot_model_architecture(model=model, file_name=file_name)\n",
    "    y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                      predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "    loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "    save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                 file_name=file_name,test_date=test_date)\n",
    "    save_train_csv(file_name=file_name)\n",
    "\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# de20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.index.values[-1] - data.index.values[0]\n",
    "days = diff.astype('timedelta64[D]')\n",
    "days = days / np.timedelta64(1, 'D')\n",
    "years = int(days/365)\n",
    "print(\"total data days:\",days)\n",
    "print(\"Total data: %d years\"%years)\n",
    "print(\"80 percent data = 1981 to %d\"%(1981 + int(0.8*years)))\n",
    "print(diff)\n",
    "delay = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#切割訓練與測試資料\n",
    "split_date = pd.Timestamp('01-01-2011')\n",
    "\n",
    "train = data.loc[:split_date]\n",
    "test = data.loc[split_date:]\n",
    "test_date = test.index\n",
    "test_date = pd.to_datetime(test_date)\n",
    "train_date = train.index\n",
    "train_date = pd.to_datetime(train_date)\n",
    "\n",
    "# 資料正規化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)\n",
    "\n",
    "print(\"train_sc.shape:\",train_sc.shape)\n",
    "\n",
    "train_sc_df = pd.DataFrame(train_sc,index=train.index,columns=train.columns)\n",
    "test_sc_df = pd.DataFrame(test_sc,index=test.index,columns=test.columns)\n",
    "\n",
    "for s in range(-delay,-delay+1):\n",
    "    train_sc_df['Y_{}'.format(s)] = train_sc_df['Close'].shift(s)\n",
    "    test_sc_df['Y_{}'.format(s)] = test_sc_df['Close'].shift(s)\n",
    "\n",
    "X_train = train_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_train = train_sc_df.dropna()['Y_-'+str(delay)]\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "\n",
    "X_test = test_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_test = test_sc_df.dropna().dropna()['Y_-'+str(delay)]\n",
    "X_test = X_test.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "\n",
    "print('Train size: (%d x %d)'%(X_train.shape[0], X_train.shape[1]))\n",
    "print('Test size: (%d x %d)'%(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neurons=[32,128,512]\n",
    "for neurons_j in neurons:\n",
    "    file_name='NYA_de'+str(delay)+'_tcn_n'+str(neurons_j)\n",
    "    print(file_name+\":\")\n",
    "    X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "    output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "            activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "            dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "    output_tensor = Dense(1)(output)\n",
    "    model = Model([input_tensor], output_tensor)\n",
    "    model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss='mse')\n",
    "    model.summary()\n",
    "\n",
    "    startTime = timeit.default_timer()\n",
    "\n",
    "    history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                        batch_size=16, verbose=1,\n",
    "                                        validation_split= val_split_ratio, callbacks=[CSV(losspath=losspath, file_name=file_name)],\n",
    "                                        shuffle=False)\n",
    "    elapsedTime = timeit.default_timer() - startTime\n",
    "    print(\"Time taken for the Network to train : \",str(datetime.timedelta(seconds=elapsedTime)))\n",
    "    plot_model_architecture(model=model, file_name=file_name)\n",
    "    y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                      predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "    loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "    save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                 file_name=file_name,test_date=test_date)\n",
    "    save_train_csv(file_name=file_name)\n",
    "\n",
    "    print(file_name+\":\")\n",
    "    custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "delays = [10,15,20]\n",
    "neurons=[32,128,512]\n",
    "for delays_i in delays\n",
    "    for neurons_j in neurons:\n",
    "        file_name='NYA_de'+str(delays)+'_tcn_n'+str(neurons_j)\n",
    "        print(\"=============================================\")\n",
    "        print(file_name)\n",
    "        Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "        file_end_with = '.csv'\n",
    "        #above need to be adjust\n",
    "        df_for_eval= pd.read_csv(Target_DirPath + file_name + file_end_with)\n",
    "        df_for_eval['Date'] = pd.to_datetime(df_for_eval[\"Date\"])\n",
    "        df_for_eval = df_for_eval.set_index([\"Date\"], drop=True)\n",
    "\n",
    "        y_pred_for_eval = df_for_eval[\"Close\"]\n",
    "        ## create empty table with label fields\n",
    "        y_pred_for_eval_data_like = np.zeros(shape=(len(y_pred_for_eval), X_train.shape[1]))\n",
    "        ## put the predicted values in the right field\n",
    "        y_pred_for_eval_data_like[:,0] = y_pred_for_eval[:]\n",
    "        ## transform and then select the right field\n",
    "        y_pred_for_eval_data = sc.transform(y_pred_for_eval_data_like)[:,0]\n",
    "\n",
    "\n",
    "        r2_test = r2_score(y_test, y_pred_for_eval_data)\n",
    "        print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "                  .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "        y_test_diff = np.diff(y_test) #y_test[i]與y_test[i-1]差異\n",
    "        y_pred_for_eval_data_diff = np.diff(y_pred_for_eval_data)#y_pred[i]與y_pred[i-1]差異\n",
    "        rsquare_product = y_test_diff*y_pred_for_eval_data_diff #兩者相乘\n",
    "        def return_same_sign_bool(d):\n",
    "            d = np.array(d)\n",
    "            return np.where(d > 0, 1, 0)\n",
    "        rsquare_product_bool = return_same_sign_bool(rsquare_product) #如果兩者相乘為正數回傳1，非正數回傳0\n",
    "        print(\"The Custom  sign score on the Test set is:\\t{:0.3f}\"\\\n",
    "                  .format((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))) \n",
    "                   #計算y_test及y_pred變動同向機率\n",
    "\n",
    "        #Custom  sign score_2\n",
    "        y_test_after = np.roll(y_test,-delay) #y_test[i]與y_test[i-1]差異\n",
    "        y_pred_for_eval_data_after = np.roll(y_pred_for_eval_data,-delay)#y_pred[i]與y_pred[i-1]差異\n",
    "        y_test__cl = np.subtract(y_test_after,y_test)\n",
    "        y_pred_for_eval_data_cl = np.subtract(y_pred_for_eval_data_after,y_pred_for_eval_data)\n",
    "\n",
    "        sign_raw = np.multiply(y_test__cl, y_pred_for_eval_data_cl)\n",
    "\n",
    "        sign_bool_func = np.vectorize(lambda elements : 1 if elements <= 0  else 0)\n",
    "        sign_bool = sign_bool_func(sign_raw)\n",
    "        sign_bool[np.r_[-delay:-1]] = 0\n",
    "\n",
    "        print(\"The Custom  sign score_2 on the Test set is:\\t{:0.3f}\"\\\n",
    "                  .format((int(sum(sign_bool)) / (len(sign_bool)-delay)))) \n",
    "                   #計算y_test及y_pred變動同向機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.constant([[[0,0],[0,1]]])\n",
    "updates = tf.constant([[[5, 5, 5, 5],[5, 5, 5, 5]]])\n",
    "shape = tf.constant([4, 4, 4])\n",
    "scatter = tf.scatter_nd(indices, updates, shape)\n",
    "print(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.constant([[[0,0],[0,1]]])\n",
    "updates = tf.constant([[5, 5]])\n",
    "shape = tf.constant([3, 5])\n",
    "scatter = tf.scatter_nd(indices, updates, shape)\n",
    "print(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = list()\n",
    "de = [0,2,6,7,8,9]\n",
    "de[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env002",
   "language": "python",
   "name": "env002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
